## 머신러닝 vs 딥러닝

- 가장 큰 차이는 Feature Extraction(추출)이다
  - 모든 Feature가 학습에 도움되지 않는다.
  - 머신러닝에서 도움이되는 Feature를 사람이 미리 선별한다.
  - 이후 학습 데이터로 넣어준다.
- 딥러닝은 자체적으로 Feature Extraction 이뤄짐.

| 머신러닝                                 | 딥러닝                                                           |
| ---------------------------------------- | ---------------------------------------------------------------- |
| AI의 한 분야(subset)                     | 머신러닝의 한 분야(subset)                                       |
| 사람이 직접 사용할 feature를 골라줘야 함 | 딥러닝 스스로 feature를 골라냄                                   |
| 적은 양의 데이터로 잘 작동               | 많은 양의 데이터가 필요                                          |
| 최소한의 하드웨어로 동작 가능            | 고성능 하드웨어(GPU 등) 필요                                     |
| 수치형 데이터만 생산 가능                | 수치형 뿐만 아니라 텍스트, 소리, 그림 등 다양한 데이터 생산 가능 |

# AI 기본 이론 - 딥러닝

- 노드를 사용해 input, hiddne, output Layer이 쌓여서 딥러닝 모델이 완성된다.
- 노드에는 학습 데이터의 feature가 들어간다.
- output으로 나온 예측값은 타겟 변수의 실제값과 비교된다.
- feature의 개수는 input 노드의 개수와 동일하다.

## hidden layer의 노드

- hidden layer에 있는 노드를 확대해보면, 가중치(weight)와 활성화 함수(activation Function)이 있다.
- 학습 과정에서 iteration 회수만큼 "가중치"를 업데이트한다. 배치 단위로 가중치를 업데이트하는 전 과정을 "학습"이라 한다.

하나의 노드 관점 (하나의 노드를 둘러싸고 벌어지는 일)

- 앞선 Layer의 노드가 n개라면, 각 노드로 들어오는 Input이 n개가 된다.
- output은 다음 노드의 Input이 된다.
- 앞 레이어에 있는 출력값이 현 레이어에 입력값이 된다.
- 각 입력값에 가중치를 곱한 시그마를 활성화 함수의 인자로 사용

대표적인 활성화 함수

- 모델의 복잡성을 높이기 위해 비선형적인 특징을 가진 활성화 함수가 도입되었다.

1. Basic non-linear function
   - Sigmoid, Tanh, Softmax
   - Sigmoid, Softmax는 분류의 출력에 주로 사용된다.
2. Advanced non-linear function
   - ReLU
   - ReLU는 주로 Hidden layer에 주로 사용된다.

Backpropagation

- output layer 이후 나온 예측값과 실제값을 비교가 끝나면 Backpropagation 실행
- 딥러닝의 학습은 역방향으로 오차가 이동하면서 각 hidden layer 노드의 가중치를 수정하는 과정
  - 오차가 줄어드는 방향으로 가중치 수정
