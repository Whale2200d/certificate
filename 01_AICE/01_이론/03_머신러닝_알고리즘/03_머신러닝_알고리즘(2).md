# 머신러닝 대표모델(1)

- 앞서 경사하강법에 대해 학습했다. 지금부터 머신러닝의 대표 알고리즘을 살펴보자.
- Basic에서는 5가지 알고리즘 학습:
  - 다중 선형 회귀(Linear Regression)
  - 로지스틱 회귀(Logistic Regression)
  - K-최근접 이웃(KNN)
  - 의사 결정 나무(Decision Tree)
  - 랜덤 포레스트(Random Forest)

## 다중 선형 회귀

- 단순 선형 회귀와 유사하게 수치를 예측하는 모델
- 다만, 여러 개의 독립변수($\beta_0$, ..., $\beta_n$)가 존재
  - 예시: 앞선 예시에서 '지각 횟수' 독립변수를 추가하면 예측 정확도가 올라갈 수 있다.
  - 일반화 수식: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... \beta_n x_n = \sum(\beta_i x_i) +\beta_0$
- 단순 선형 회귀와 마찬가지로, MSE를 목적함수로 사용해 경사하강법으로 학습할 수 있다.

✅ Tip. 스케일링은 왜 하는가?

- Min-Max Scaling, Standard Scaling
- 독립변수가 늘어날수록, $\beta_n$ 파라미터의 수도 늘어난다.
- 개별 $\beta$별로 편미분을 실행해야 함.
- 경사하강법에 의한 기울기 감소 정도는 각 데이터의 스케일(Scale, 크기)에 따라 달라진다.
- 컬럼별로 데이터 크기가 다르므로, 데이터별로 기울기 감소 폭이 제각각이다. (예시: 연봉은 0000만이지만, 나이는 00살이므로, 데이터 크기가 다르므로, 편미분 폭도 달라짐.)
- 기울기 감소 폭이 달라지면, 전체 학습 속도가 느려지거나, 학습을 실패하는 문제가 발생한다.
- 따라서 모든 $\beta$에 대해 기울기가 감소하는 속도를 동일하게 가져가야 한다.
- 컬럼 데이터의 크기를 비슷하게 맞춰 기울기 감소 속도를 유시하게 만드는 방법이 필요하다.
- 이런 이유로 스케일링 작업을 통해 데이터의 크기를 유사하게 맞춘다.
- 일반적으로 Standard Scaling을 많이 사용함.

## 로지스틱 회귀

- 이름은 회귀지만, 회귀가 아닌 **분류를 위한 알고리즘**
- 시그모이드 함수($y = 1 / (1 + e^{-x})$)를 사용하기 떄문에 분류 알고리즘으로 사용
- 시그모이드 함수는 0과 1사이에서 움직이는 함수라서, 확률에서 사용
- 확률로 해석하기 때문에 실제로는 분류를 위한 방법으로 사용
- 예시:
  - A, B를 분류하는 문제라면
  - 분류의 기준이 되는 확률인 $Threshold = 0.5$를 기준으로 크냐 작냐에 따라 A, B를 판단할 수 있다.

- 로지스틱 회귀는 다중 선형 회귀 모델을 입력값으로 갖는 모델이다.
  - $z = \sum(\beta_i x_i) +\beta_0$
  - $시그모이드 함수 = 1 / (1 + e^{-z})$ (x 대신 z 사용)

- 예시: 공부시간을 토대로 합격 여부를 판단하는 문제로 변형 가능
  - $Threshold = 0.5$
  - $Threshold >= p$일 경우, 합격
  - $Threshold < p$일 경우, 불합격

    (Threshold도 조정할 수 있으므로, 모델 성능이 좋아지는 방향으로 조정 가능하다.)

- 로지스틱 회귀 또한 앞서 학습한 선형 회귀처럼 목적함수(또는 비용함수)를 최소화시키도록 경사하강법을 적용한다.
- 선형 회귀에서는 MSE를 비용함수로 사용했다면, **로지스틱 회귀에서는 NLL(Negative Log Likelihood)를 비용함수**로 사용

## K-최근접 이웃

- 이하 KNN
- KNN은 서로 근접한 데이터가 비슷하다는 가정에서 출발한다.
- 예측하려는 데이터로부터 가장 거리가 가까운 k개의 최근접 데이터를 참조하는 알고리즘이다.
- 데이터 준비 &rarr; 데이터 간 거리 계산 &rarr; 최근접 k개 데이터 선정 &rarr; 예측
  - 예시: k=3이고, B가 2개, A가 1개라면 다수결에 의해 예측하려는 데이터를 B로 분류

- 유클리드 거리 계산: $distance = \sqrt{(공부시간_{예측} - 공부시간_{비교})^2 + (지각횟수_{예측} - 지각횟수_{비교})^2}$
- 회귀와 분류에 어떻게 적용할 수 있을까?
  - k=3이라면, 유클리드 거리 상 가장 가까운 3개의 비교군을 선발
  - (회귀) 3명의 시험 성적 **평균**을 구하면 이게 예측값이 된다.
  - (분류) 3명의 합/불합 **최빈값**을 구하면 이게 예측값이 된다.

# 마무리(Wrap-up)

- 다중 선형 회귀 모델은 선형 모델을 이용한 가장 단순한 회귀 알고리즘, 비용함수로 MSE를 사용, 단순한 만큼 학습 속도가 빠르지만 복잡한 상황에서는 성능이 낮을 수 있다.
- 로지스틱 회귀는 이름과 다르게 분류에 사용하는 알고리즘, 비용함수로 NLL을 사용, Threshold 값을 어떻게 사용하냐에 따라 예측 결과가 달라질 수 있다.
- KNN은 근처 데이터와 거리를 계산하여, 근처 k개의 데이터를 이용해 예측하는 알고리즘, **비지도학습**의 대표 알고리즘
