# 학습 목표

- 앞서 배운 GD 알고리즘의 확장 모델에 대해 알 수 있다.
- 딥러닝 학습에 사용하는 Hyperparameter에 어떤 게 있는지, 어떻게 설정하는지에 대해 알 수 있다.
- 학습 과정에서 발생할 수 있는 문제가 무엇이고, Overfitting에 대해 알 수 있다.

## 딥러닝 학습방법

가중치 업데이트: 경사하강법(Gredient Descent)

- 단계적 접근방식을 취하여 조금씩 개선하며 최적화하는 방법
- 현재 weight에서 loss가 줄어드는 방향
- loss는 MSE, MAE 사용할 수 있으나, 미분하기 위해 MSE 사용
- loss의 기울기값의 반대방향으로 이동하면 OK
- Learning Rate: 기울기값보다 작게 이동
  - $W = W - \alpha \delta L / \delta W$
  - $\alpha$를 Learning Rate

## Learning Rate

- Weight 업데이트 움직임의 크기
- 보폭이 너무 작다면, 학습이 느려짐
- 보폭이 너무 크다면, 최적화 불가능
- 적절한 보폭(Learning Rate)가 필요
  - 특정 상수값을 사용하거나
  - 스케줄링 등을 통해 점차 줄어들게 설정

## Optimization Algorithm

- GD 방법을 토대로 Step의 방향 또는 Step의 사이즈를 개선하는 쪽으로 최적화 알고리즘이 발전해옴
- Step의 사이즈는 보폭을 의미한다.
- 대표적인 방법: SGD, ADAM

SGD(Stochastic GD)

- SGD는 batch data(size=1)에 대해 Gradient 계산하여 빠르게 움직임
- 기존 GD는 전체 데이터를 보고 한 걸음씩 걸어가는 반면
- SGD는 데이터 하나를 보고 빠르게 걸어간다.
- mini-batch SGD도 있다.
  - batch size가 2이상
  - 기존 SGD는 batch size가 1이라서 자주 틀어짐.

ADAM

- 기존 방향이 이동하던 방향대로 관성에 따라 Step을 이동(Momentum)
- 각 Feature별 기울기의 개선정도를 반영하여 이동하는 Step의 크기를 보정(Adaptive)
  - 개선이 많이 된 건 조금만
  - 개선이 적게 된 건 많이
  - weight 간의 개선 맞춰가며 성능을 극대화시킴

## 용어 정리

Batch size, Epoch

- 1 Epoch: 모든 데이터 셋을 한 번 학습
- Minibatch: 데이터 셋을 batch size 크기로 쪼개서 학습
- 1 iteration: mini batch 데이터 셋의 1회 학습

Activation Function

- 시그모이드 함수
  - 주로 이진 분류에 사용
  - 기울기 소실(Gradient Vanishing) 현상 발생할 수 있음.
    - 히든 레이어 단계가 많아질수록, 발생함.
    - 시그모이드 함수의 기울기가 0.25이고, 히든 레이어는 미분값의 곱으로 계산되므로, 히든 레이어가 많아질수록 $0.25^n$이 기울기가 되므로 기울기 소실 문제가 발생함. (Gradient 소멸 &rarr; update 안됨 &rarr; 히든 레이어가 깊은 구조가 의미가 없어짐)

  - 분류에서 사용되는 함수
    - 시그모이드 함수, Softmax 함수
    - 시그모이드: y=0.5를 기준으로 확률값 맵핑
    - Softmax: 개별 $y_n$에 대해 확률값 맵핑
      - 시그모이드가 이중 분류에만 사용하므로 등장한 함수
      - 다중 분류인 확률값이므로, 모두 합치면 1이 됨
    - 분류 문제는 수치로 끝나는 게 아니라, Class로 정의를 끝내야 하기 때문에, Class에 속하는 확률값을 계산해주는 함수임.

- ReLu 함수
  - 기울기 소실 문제를 해결하기 위해 ReLU 함수 등장
  - 0 이상일 때 x값 그대로 가져가므로 기울기 소실 문제 해결됨

## Train Test Split

- 학습데이터 / 평가데이터 분리
- 학습, 검증, 평가는 7:1:2 비율로 분리해서 사용
- 평가 데이터(Valid): 학습 결과 중간 검증 및 **하이퍼파라미터 튜닝 검증**용 Data

## Under / Ideal / Over Fitting

- AI Modeling의 목표는 범용적인 데이터를 잘 예측할 수 있는 모델을 생성하는 것이다.
- Under Fitting: 학습을 너무 적게 진행해서, 예측값을 제대로 뽑아낼 수 없는 상태
- Ideal Fitting: 범용적인 데이터를 잘 예측할 수 있는 상태
- Over Fitting: 너무 많이 학습을 진행해서, 내 학습데이터의 특성에만 딱 떨어지게 학습한 상태

## Over Fitting 방지 기법

1. Dropout

- 히든 레이어 간 Fully Connected된 노드의 일부를 Dropout 시킴

2. Early Stopping

- Train loss는 학습이 진행될수록 줄어듦
- Valid loss는 학습이 진행될수록 줄어들다가, 다시 증가함.
- 다시 증가하기 전에 학습 중지하는 방법

3. 모델을 단순하게, 데이터를 추가 수집

- 모델이 깊어질수록 오버피팅이 나타날 수 있으므로

# 마무리

1. Optimization 알고리즘에는 GD 외 SGD, ADAM 등이 있다.
2. Learning Rate 설정을 통해 업데이트할 보폭을 결정한다.
3. Train-Test Split을 통해 데이터를 분할한다.
4. 오버피팅을 방지하기 위한 기법으로 Dropout, Early stopping이 있다.
